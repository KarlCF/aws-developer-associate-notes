# AWS Developer Exam Notes

These are my notes while studying for the AWS Certified Developer Exam, I used the Stephane Mareek udemy course:
 <https://www.udemy.com/course/aws-certified-developer-associate-dva-c01/>

The objective of writing along with the course and making it public is to help others learn and follow through the same content.

To do:

1. Review notes after finishing the course
2. Add links to segments and AWS Faqs links to each Service
3. Add notes after taking the practice exam and certification exam

## AWS Fundamentals Part 1: IAM + EC2

## **EC2**

### **SSH into linux: chmod 0400 ec2keypair.pem**

* ssh -i ec2keypair.pem ec2-user@255.255.255.255
* It's good to maintain one separate security group for SSH access (best practice)
* Limit of 5 Elastic IP's per account (if you need to increase, just contact AWS)
* Try to avoid using Elastic IP:
  * They often reflect poor architectural decisions
  * Instead, use a random public IP and register a DNS name on it
  * Use a load balancer and no public IP (best pattern)
* Bootstrapping = EC2 User Data (only runs once at the first start)
* EC2 User Data is automaticaly run with the sudo command

## **AWS Fundamentals Part 2: ELB + ASG + EBS**

## Load balancers

* Application Load Balancer (v2)
  * Load balancing multiple HTTP application accros machines (target groups)
  * Load balancing to multiple applications on the same machine (ex: containers)
  * Load balancing based on route in URL
  * Load balancing based on hostname URL
  * Has a port mapping feature to redirect to a dynamic port
  * Great for micro services & container-based application (ex: Docker & Amazon ECS)

  * ### **Good to know:**

  * Stickiness can be enabled at the target group level
    * Same requests to the same instance
    * Stickiness is directly generated by the ALB (not the application)
  * ALB supports HTTP/HTTPS & Websockets protocols
  * The application servers don't see the IP of the client directly
    * The true IP of the client is inserted in the header X-Forwarded-For
    * We can also get Port (X-Forwarded-Port) and proto (X-Forwarded-Proto)

* Network Load Balancer (v2)
  * Forward TCP traffict to your instances
  * Handle millions of requests per seconds
  * Support for static IP or elastic IP
  * Less latency: ~ 100 ms (vs 400 ms for ALB)
  * Mostly used for extreme performance, should not be the default load balancer you chose

### **Good to know:**

* Classic Load Balancers are Depreacted
  * ALB for HTTP / HTTPs & Websocket
  * Network Load Balancer for TCP
* CLB and ALB support SSL certificates and provide SSL termination
* All Load Balancers have health check capability
* ALB can route based on hostname / path
* ALB is a great fit with ECS (Docker)
* Any Load Balancer (CLB, ALB, NLB) has a static host name. Do not resolve and use underlying IP
* LBs can scale but not instantaneously - contact AWS beforehand for a "warm-up"
* NLB directly see the client IP
* 4xx errors are client induced errors
* 5xx errors are application induced errors
  * Load Balancer Errors 503 means at capacity or no registered target
* If the LB can't connect to your application, check your security groups!
* Monitoring
  * ELB access logs will log all access requests (so you can debug per request)
  * CloudWatch Metrics will give you aggregate statistics (ex: connections count)

## Auto Scaling Group

* Handles scaling (in and out) to a minimum and maximum configured
* Automatically register new instances to a load balancer
* It is possible to scale an ASG based on CloudWatch alarms

### ASGs have the following attributes

* A launch configuration
  * AMI + Instance type
  * EC2 User data
  * EBS Volumes
  * Security Groups
  * SSH Key Pair
* Min Size / Max Size / Initial Capacity
* Network + Subnets Information
* Load Balancer Information (Or target group info)
* Scaling Policies

### Auto Scaling new Rules

* It is now possible to define more refined auto scale rules that are directly managed by EC2
  * Target Average CPU Usage
  * Number of requests on the ELB per instance
  * Average Network In
  * Average Network Out
* These rules are easier to set up and can make more sense

### **Good to Know:**

* Scaling policies can be any CloudWatch metrics (Native or custom) and even based on schedule
* ASGs use Launch configurations and you update an ASG by providing a new launch configuration
* IAM roles attached to an ASG will get assigned to EC2 instances
* Instances marked as Unhealthy are terminated by the ASG and replaced

___

## EBS Volumes

* It's a network drive
  * It uses network to communicate the instance, so there might be some latency
  * It can be detached from one instance and attached to another quickly
* It's locked to an **AZ**
  * An EBS from AZ A cannot be attached to AZ B, to move accros you first need to snapshot it
* Have provisioned capacity (in size and IOPS) and you are billed accordingly (not for the use, but for the provision by itself)
* EBS Volumes come in 4 types:
  * GP2 (SSD): General purpose SSD volume that balances price and performance, various workloads
  * IO1 (SSD): Highest-performance SSD volume for mission-critical low-latency or high-throughput workloads
  * ST1 (HDD): Low cost HDD volume designed for frequently accessed, throughput intensive workloads
  * SC1 (HDD): Lowest cost HDD volume designed for less frequently accessed workloads
* EBS are characterized in Size | Throughput | IOPS
* As of Feb. 2017, you can resize EBS volumes
  * Size (any volume type)
  * IOPS (only IO1)
  * After resizing an EBS volume, you need to repartition your driver on the EC2 instance

* ### EBS Encryption

* When you create an encrypted EBS volume, you'll get the following:
  * Data at rest is encrypted inside the volume
  * All the data in flight (moving between instance and volume) is encrypted
  * All Snapshots are encrypted
  * All volumes created from snapshots are also encrypted
  * Encryption and decryption are transparent (no user interaction is needed)
  * Encryption has minimal iompact on latency
  * EBS Encryption utilizes key from KMS (AES-256)
  * Copying an unencrypted snapshot allows encryption

* ### EBS Snapshots

* EBS can be backed up using snapshots
* Snapshots only take the used space on your EBS volume
* Snapshots are used for:
  * Backups (disaster recovery)
  * Volume migration
    * Resizing a volume down
    * Changing the volume type
    * Encrypt a volume

## **AWS Fundamentals Part 3: Route 53 + ElastiCache + VPC**

## Route 53

* Most common records are:
  * A: URL to IPv4
  * AAAA: URL to IPv6
  * CNAME: URL to URL
  * Alias: URL to AWS resource
* Route53 can use:
  * Public domain names you own
  * Private domain names that can be resolved by your VPCs. (ex: application1.company.internal)
* Route53 has advanced features such as:
  * Load Balancing (Through DNS - also called client load balancing)
  * Health checks (although limited...)
  * Routing policy: simple, failover, geolocation, geoproximity, latency, weighted
* Prefer Alias over CNAME for AWS resources (bettter performance)

## RDS

* RDS Read Replicas for read scalability
  * Up to 5 Read Replicas
  * Within AZ, Cross AZ or Cross Region
  * Replication is ASYNC, so reads are eventually consistent
  * Replicas can be promoted to their own DB
  * Applications must update the connection string to leverage read replicas
* RDS Multi AZ (Disaster Recovery)
  * SYNC replication
  * One DNS name - automatic app failover to standby
  * Increases availability
  * Failover in case of loss of AZ, loss of network, instance or storage failure
  * No manual intervention in apps
  * Not used for scaling
* RDS Backups
  * Automatically enabled in RDS
  * Automated backups:
    * Daily full snapshot of the database
    * Capture transaction logs in real time => ability to restore to any point in time
    * 7 days retention (default) that can be increased to 35 days
  * DB Snapshots:
    * Manually triggered by the user
    * Retention of backup for as long as you want
* RDS Encryption
  * Encryption at rest capability with AWS KMS - AES-256 encryption
  * SSL certificates to encrypt data to RDS in flight
  * To enforce SSL:
    * PostgreeSQL: rds.force_ssl=1 in AWS RDS Console (Parameter Groups)
    * MySQL: Within the DB:
    GRANT USAGE ON \*.\* TO 'mysqluser'@'%' **REQUIRE SSL**;
  * To connect using SSL:
    * Provide the SSL Trust certificate (download from AWS)
    * Provide SSL options when connecting to database
* RDS Security
  * RDS databases are usually deployed within a private subnet, not in a public one
  * RDS Security works by leveraging security groups - it controls who can communicate with RDS
  * IAM policies help control who can manage AWS RDS
  * Traditional Username and Password can be used to login to the database
  * IAM users can now be used too (MySQL / Aurora - New feature)
* RDS vs Aurora
  * Aurora is proprietary technology from AWS (not open sourced)
  * Postgres and MySQL are both supported as Aurora DB (your drivers will work as if Aurora was a Postgres or MySQL database)
  * Aurora is "AWS cloud optimized" and claims 5x performance improvement over MySql on RDS and 3x over Postgres on RDS
  * Aurora's storage automatically grows of increments of 10GB, up to 64 TB
  * Can have 15 replicas, while MySQL has 5, and the replication is faster (below 10 ms replica lag)
  * Failover in Aurora is instantaneous. It's High Available natively
  * Costs more than RDS (about 20%), but more efficient

## ElastiCache

* Managed Redis or Memcached
* Caches are in-memory databases with high performance, low latency
* Reduce load off of databases for read intensive workloads
* Helps make your application stateless
* Write Scaling using sharding
* Read Scaling using Read Replicas
* Multi AZ with Failover Capability
* AWS manages OS maintenance / patching optimizations, setup, configuration, monitoring, failure recovery and backups
  * ElastiCache Solution Architecture - DB Cache
  * Applications queries ElastiCache, if not available, gets the data from RDS and writes it back to ElastiCache, making future reads quicker
  * Helps relieve load in RDS
  * Cache must have an invalidation strategy to make sure that only the latest data is used there (is done in the application side)
* Redis Overview
  * Is a in-memory key-value store
  * Low latency (under ms)
  * Cache survive reboots by default (it's called persistence)
  * Great to host
    * User sessions
    * Leaderboard (for gaming)
    * Distributed states
    * Relieve pressure on databases (such as RDS)
    * Pub / Sub capability for messaging
  * Multi AZ with Automatic Failover for disaster recovery if you don't want to lose your cache data
  * Support for Read Replicas
* Memcached Overview
  * Is an in-memory object store
  * Cache doesn't survive reboots
  * Use cases:
    * Quick retrieval of objects from memory
    * Cache often accessed objects
  * ***Non exam notes:*** Overall, Redis has better feature sets and has grown in popularity.
* ElastiCache Patterns
  * ElastiCache is helpful for read-heavy application workloads
    * Social networks, gaming, media sharing, Q&A portals
  * *compute-intensive* workloads (recommendation engines)
  * There are two pattern / cache strategies for ElastiCache (may be different based on the kind of application you have)
    * Lazy Loading - Load only when necessary
      * Pros:
        * Only requested data is cached (cache isn't filled with unused data)
        * Node failures aren't fatal (just increased latency on to "warm" the cache)
      * Cons:
        * Cache miss results in 3 round trips, noticeable delay for that request
        * Stale data: data can be updated in the database and outdated in the cache (one way you can solve this by applying a ttl in the cache)
    * Write Through - Add or Update cache whenever database is updated
      * Pros:
        * Data is never stale in the cache
        * Write penalty vs Read penalty (each write requires 2 calls)
      * Cons:
        * Missing Data until it is added / updated in the DB. Can be mitigated by implementing Lazy Loading strategy together
        * Cache churn - a lot of data that is rarely read, causing the cache size to be sometimes as big as the DB

## VPC

* Public Subnets usually contains:
  * Load Balancers
  * Static Websites
  * Files
  * Public Authentication Layers
* Private Subnets usually contains:
  * Web application servers
  * Databases
* Public and private subnets can communicate if they're in the same VPC
* It's possible to use a VPN to connect to a VPC
* VPC Flow Logs allow you to monitor the traffic within, in and out of your VPC

## **AWS Fundamentals Part 4: S3**

* Buckets are defined at the region level
* Must have globally unique names
* Objects (files) have a Key. The key is the ***FULL*** path:
  * <my_bucket>/my_file.txt
  * <my_bucket>/my_folder1/another_folder/my_file.txt
* There's no concept of "directories" within buckets (althought the UI might make it appear like there is), just keys with very long names that contain slashes ("/")
* Object Values are the content of the body:
  * Max Size is 5TB
  * If uploading more than 5GB, must use "multi-part upload"
* Objects come with Metadata (list of text key / value pairs - system or user metadata)
* Tags (Unicode key / value pair - up to 10) - useful for security / lifecycle
* Version ID (if versioning is enabled)

### AWS S3 - Versioning

* Versioning is enabled at the bucket level
* It's best practice to version your buckets
  * Protect against unintended deletes
  * Easy roll back to previous version
* Any file not versioned prior to enabling versioning will have version "null"

### S3 Encryption

* There are 4 methods of encrypting objects in S3:
  * SSE-S3: encrypts S3 objects using keys handled & managed by AWS
    * Object is encrypted server side
    * AES-256 encryption type
    * Must set header: "x-amz-server-side-encryption":"AES256"
  * SSE-KMS: leverage AWS KMS to manage encryption (can use CMK)
    * Advantages: user control + audit trail
    * Object is encrypted server side
    * Must set header: "x-amz-server-side-encryption":"aws:kms"
  * SSE-C: when you use your own keys to encrypt data
    * Amazon S3 does not store the encryption key you provide
    * **HTTPS must be used**
    * Encryption key must be provided in HTTP headers, for every HTTP request made
  * Client Side Encryption: data is encrypted client side
    * Client use a library such as the Amazon S3 Encryption Client
    * Clients must encrypt data themselves before sending to S3
    * Clients must decrypt data themselves when retrieving from S3
    * Customer fully manages the key and encryption cycle
* Encryption in transit (SSL):
  * AWS S3 exposes:
    * HTTP endpoint: non encrypted
    * HTTPS endpoint: encryption in flight
  * You're able to use the endpoint you want, but HTTPS is reccomended
  * HTTPS is mandatory for SSE-C
  * Encryption in flight is also called SSL / TLS

* ### S3 Security & Bucket policies

* User based:
  * IAM policies - which API calls should be allowed for a specific user from IAM console
* Resource based:
  * Bucket Policies - bucket wide rules from S3 console - allows cross account
  * Object Access Control List (ACL) - finer grain
  * Bucket Access Control List (ACL) - less common

### S3 Bucket Policies

* JSON based policies
  * Resources: buckets and objects
  * Actions: Set of API to Allow or Deny
  * Effect: Allow / Deny
  * Principal: The account or user to apply the policy to
* We can use S3 bucket policy to:
  * Grant public access to the bucket
  * Force objects to be encrypted at upload
  * Grant access to another account (Cross Account)

### S3 Security

* Networking:
  * Supports VPC Endpoints (for instances in VPC without internet)
* Logging and Audit:
  * S3 access logs can be stored in other S3 bucket
  * API calls can be logged in AWS CloudTrail
* User Security:
  * MFA can be required in versioned buckets to delete objects
  * Signed URLs: URLs that are valid only for a  limited time

### S3 Websites

* If you get a 403 (Forbidden) error, check if the bucket policy allows public reads

### S3 CORS

* Cross Origin Resource Sharing allows you to limit the number of websites that can request your files in S3 (and limit your costs)
* If you request data from another S3 bucket, you need to enable CORS

### S3 - Consistency Model

* **Read after write consistency for PUTS of new objects**
  * As soon as an object is written, it can be retrieved.
  ex: (PUT 200 -> GET 200)
  * The exception is that if a GET is done before the object exists, the result will be cached and after writing the object it will be *eventually consistent*
  ex: (GET 404 -> PUT 200 -> GET 404)
* **Eventual Consistency for DELETES and PUTS of existing objects**
  * If we read an object after updating, we might get the older version
  * If we delete an object, we might still be able to retrieve it for a short time.

### S3 - Performance

* Before July 17th 2018
  * When you had > 100 TPS (transactions per second) S3 performance could degrade
  * Behind the curtains, each object goes to an S3 partition and for the best performance, we want the highest partition distribution
  * ***In the exam and in the past:*** It is reccomended to have random prefix in front of your key name to optimize perfomrance (never rerccomended to use date prefixes, as the pertitions would be very similar)
* After July 17th 2018 (Current state, but not in exam)
  * We can scale up to 3500 RPS for PUT and 5500 RPS for GET for EACH PREFIX
  * For reference: <https://aws.amazon.com/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/>
  * As stated in the news: 'This S3 request rate performance increase removes any previous guidance to randomize object prefixes to achieve faster performance.'
* Faster upload of large objects (>=100mb), use multipart upload:
  * Parallelizes PUTs for greater throughput
  * Maximize your network bandwidth and efficiency
  * Decrease time to retry in case a part fails
  * MUST use a multi-part if object size is greater than 5GB
* Use CloudFront to cache S3 objects around the world
* S3 Transfer Acceleration (uses edge locations) - just needs to change the endpoint you write to, not the code
* If using SS3-KMS encryption, you may be limited to your AWS limits for KMS usage (~100s - 1000s downlaods / uploads per second)

### S3 & Glacier Select

* If you retrieve data in S3 and Glacier, you may only want a **subset** of it
* If you retrieve all the data, the network cost may be high
* WIth S3 Select / Glacier Select, you can use SQL **SELECT** queries to let S3 or Glacier know exactly which attributes / filters you want
  * select * from s3object s where s.\"Country (Name)\" like %United States&'
* Save cost up to 80% and increase performance by up to 400%
* The "SELECT" happens "within" S3 or Glacier
* Works with files in CSV, JSON or Parquet format
* Files can be compressed with GZIP or BZIP2
* No subqueries or Joins are supported

___

## Developing in the AWS: AWS CLI, SDK, IAM Roles & Policies

## AWS CLI

* AWS CLI on EC2 instances: Never place your credentials on EC2, always use IAM Roles attached to it
* Some AWS CLI commands (not all) conmtain a --dry-run option to simulate API calls
* When API calls fail, you can get a long error message, which can be decoded using the **STS** command line(IAM user or Role must have STS write permission: decode message):
  * sts **decode-authorization-message**
* EC2 Instance Metadata:
  * Allows EC2 instances to "learn about themselves" without using an IAM Role for that purpose
  * The URL is <http://169.254.169.254/latest/meta-data> (ex: `curl http://169.254.169.254/latest/meta-data`)
    * You can retrieve IAM Role name from the metadata, but you ***cannot*** retrieve the IAM Policy
  * Metadata = info about the EC2 Instance
  * Userdata = launch script of the EC2 Instance
* AWS Profile
  * You can use `aws configure --profile` to configure multiple credentials on the same machine
  * When running commands, they will still run on the default profile, if you need to use an specific profile you should finish the command with `--profile {your-profile}`

## AWS SDK overview

* When you want perform actions on AWS directly from your application without using the CLI, you can use an SDK (software development kit)
* We have to use AWS SDK when coding against AWS Services such as DynamoDB
* If you don't specify or configure a default region, then us-east-1 will be chosen by default
* It's recommended to use the **default credential provider chain**
  * The **default credential provider chain** works seamlessly with:
    * AWS credentials at ~/.aws/credentials (only on our machine / on premises)
    * Instance Profile Credentials using IAM Roles
    * Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
  * ***NEVER STORE AWS CREDENTIALS ON YOUR CODE***
* Any API that fails because of too many calls needs to be retried with Exponential Backoff
  * These apply to rate limited API
  * Retry mechanism included in SDK API calls

___

## **AWS Elastic Beanstalk**

### Elastic beanstalk overview

* ElasticBeanstalk is a developer centric view of deploying applications on AWS
* It uses all the components we've seen before:
  * EC2, ASG, ELB, RDS, etc...
* We still have full control over the configuration
* BeanStalk is free but you will pay for the underlying resources
* Managed service
  * Instance configuration / OS is handled by beanstalk
  * Deployment strategy is configurable but performed by ElasticBeanstalk
* Just the application code is responsibility of the developer
* Three architecture models:
  * Single Instance deployment: good for dev
  * LB + ASG: great for production or pre-production web applications
  * ASG only: great for non-web apps in productions (workers, etc...)
* ElasticBeanstalk has three components:
  * Application
  * Application version: each deployment gets assigned a version
  * Environment name (dev, test, prod...): free naming
* You deploy application versions to environments and can promote application versions to the next environment
* Rollback feature to previous application version
* Full control over lifecycle of environments
* Support for many platforms, and if not support you can write your custom platform (advanced)

### Elastic Beanstalk Deployment Modes

* Single Instance - Great for Dev
* High Availability with Load Balancer - Great for Prod

### Beanstalk Deployment Options for Updates

* All at once (deploy all in one go) - Fastest, but instances aren't available to serve traffic for a bit (has downtime)
  * Fastest deployment
  * Application has downtime
  * Great for quick iterations in development environment
  * No additional cost
* Rolling: Update a few instances at a time (also called buckets), and then move onto the next bucket once the first bucket is healthy
  * Application is running below capacity
  * Can set the bucket size
  * Application is running both versions simultaneously
  * No additional cost
  * Long deployment
* Rolling with additional batches: like rolling, but spins up new instances to move the batch (so that old application is still available and at full capacity)
  * Application is still rolling at full capacity
  * Can set the bucket size
  * Application is running both versions simultaneously
  * Small additional cost
  * Additional batch is removed at the end of the deployment
  * Longer deployment
  * Good for prod
* Immutable: spins up new instances in a new ASG, deploys version to these instances, and then swaps all the instances when everything is healthy
  * New Code is deployed to new instances on a temporary ASG
  * High cost, double capacity
  * Longest deployment
  * Quick rollback in case of failures (just terminate new ASG)
  * Great for Prod

### Elastic Beanstalk Deployment Blue / Green

* Not a "direct feature" of Elastic Beanstalk
* Zero downtime and release facility
* Create a new "stage" environment and deploy v2 there
* The new environment (green) can be validated independently and roll back if issues occur
* Route53 can be setup using weighted policies to redirect a little bit of traffict to the stage environment
* Using Beanstalk, "swap URLs" when done with the environment test

### Elastic Beanstalk Extensions

* A zip file containing your code must be deployed to Elastic Beanstalk
* All the parameters set in the UI can be configured with code using files
  * Requirements:
    * In the .ebextensions/ directory in the root of the source code
    * YAML / JSON format
    * .config extensions (example: logging.config)
    * Able to modify some default settings using: option_settings
    * Ability to add resources such as RDS, ElastiCache, DynamoDB, etc...

### Elastic Beanstalk CLI

* We can install an additional CLI called the "EB cli" which makes working with Beanstalk from the CLI easier
* Basic commands are:
  * eb create
  * eb status
  * eb health
  * eb events
  * eb logs
  * eb open
  * eb deploy
  * eb config
  * eb terminate
* Very helpful when automating deployment pipelines

### Elastic Beanstalk under the hood

* Elastic Beanstalk relies on CloudFormation

### Elastic Beanstalk Deployment Mechanism

* Describe dependencies (requirements.txt for Python, package.json for Node.js)
* Package code as zip
* Zip file is uploaded to each EC2 machine
* Each EC2 machine resolves dependencies (slow if there are a lot of dependencies or if they are big)
* Optimization in case of long deployments: Package dependencies with source code to improve deployment performance and speed

### Elastic Beanstalk Exam Tips

* Beanstalk can work with HTTPS
  * Idea: Load the SSL onto the Load Balancer
  * Can be done from the Console ( EB console, load balancer configuration)
  * Can be done from the code: .ebextensions/securelistener-alb.config
  * SSL Certificate can be provisioned using ACM (AWS Certificate Manager) or CLI
  * Must configure security group to allow incoming port 443 (HTTPS port)
* Beanstalk redirect HTTP to HTTPS
  * Configure your instances to redirect HTTP to HTTPS
  * Or configure Application Load Balancer (ALB only) with a rule
  * Make sure health checks are not redirected (so they keep sending 200 OK)
* Elastic Beanstalk Lifecycle Policy
  * Elastic Beanstalk can store at most 1000 application versions
  * If you don't remove old versions, you won't be able to deploy newer ones
  * To phase out old application versions, use a lifecycle policy
    * Based on time (old versions are removed)
    * Based on space (when you have too many versions)
  * Versions that are currently used won't be deleted
  * Even if the version is deleted, you have the option not to delete the source bundle in S3 to prevent data loss
* Web Server vs Worker Environment
  * If your application tasks that are long to complete, offload these tasks to a dedicated worker environment
  * Decoupling your application into two tiers in common
    * Example: processing a video, generating a zip file, etc
  * You can define periodic tasks in a file cron.yaml
* RDS with Elastic Beanstalk
  * Rds can be provisioned with Beanstalk, which is great for dev / test, but not justifiable for Prod as it can result in data loss
  * The best for prod is to separately create an RDS database and provide our EB application with the connection string
  * Steps to migrate from RDS coupled in EB to standalone RDS:
    * Take an RDS DB Snapshot
    * Enable deletion protection in RDS
    * Create a new environment without an RDS, pointing to the existing old RDS
    * Perform blue/green deployment and swap new and old environments
    * Terminate the old environment (RDS won't be deleted, as the deletion protect is activated)
    * Delete CloudFormation stack (will be in DELETE_FAILED state, as the RDS still remains)

___

## **AWS CI/CD: CodeCommit, CodePipeline, CodeBuild, CodeDeploy

* AWS CodeCommit: Storing our code
* AWS CodePipeline: automating our pipeline from code to ElasticBeanstalk (?)
* AWS CodeBuild: building and testing our code
* AWS CodeDeploy: deploying the code to EC2 fleets (not Beanstalk)

### Codemmit Overview

* Private Git repositories
* No size limit on repositories (scale seamlessly)
* Fully managed, highly available
* Code only in AWS Cloud account => increased security and compliance
* Secure (Encryption, access control, etc...)
* Integrated with Jenkins / CodeBuild / and other CI tools

### CodeCommit Security

* Interactions are done using Git (standard)
* Authentication in Git:
  * SSH Keys: AWS Users can configure SSH keys in their IAM Console
  * HTTPS: Done through the AWS CLI Authentication helper or Generating HTTPS credentials
  * MFA (multi factor authentication) can be enabled for extra safety
* Authorization in Git:
  * IAM Policies manage user / roles rights to repositories
* Encryption:
  * Repositories are automatically encrypted at rest using KMS
  * Encrypted in transit (can only use HTTPS or SSH - both secure)
* Cross Account access:
  * Use IAM Role in your AWS Account and use AWS STS (with AssumeRole API)

### CodeCommit vs GitHub

* Similarities:
  * Both are git repositories
  * Both support code review (pull requests)
  * GitHub and CodeCommit can be integrated with AWS CodeBuild
  * Both support HTTPS and SSH method of authentication
* Differences:
  * Security:
    * User administration:
      * GitHub: GitHub Users
      * CodeCommit: AWS IAM users & roles
  * Hosted:
    * GitHub: hosted by GitHub
    * GitHub Enterprise: self hosted on your servers
    * CodeCommit: managed & hosted by AWS
  * UI:
    * GitHub Ui is fully featured
    * CodeCommit UI is minimal

### CodeCommit Notifications

* You can trigger notifications in CodeCommit using AWS SNS, AWS Lambda or AWS CloudWatch Event Rules
* Use cases for SNS / AWS Lambda notifications:
  * Deletion of branches
  * Trigger for pushes that happens in master branch
  * Notify external Build System
  * Trigger AWS Lambda function to perform codebase analysis (maybe credentials got commited to the code)
* Use cases for CloudWatch Event Rules:
  * Trigger for pull request updates (created / updated / deleted / commented)
  * Commit comment events
  * CloudWatch Event Rules goes into an SNS Topic

### CodePipeline Overview

* Continuous delivery
* Visual Workflow
* Source: GitHub / CodeCommit / Amazon S3
* Build: COdeBuild / Jenkins / Etc.
* Load Testing: 3rd party tools
* Deploy: AWS CodeDeploy / Beanstalk / CloudFormation / ECS ...
* Made of Stages:
  * Each stage can have **sequential actions and/ or parallel actions**
  * Stage examples: Build / Test / Deploy / Load Test /etc...
  * Manual approval can be defined at any stage

### AWS CodePipeline Artifacts

* Each pipeline stage can create "artifacts"
* Artifacts are passed stored in Amazon S3 and passed on to the next stage

### CodePipeline Troubleshooting

* CodePipeline state changes happen in **AWS CloudWatch Events**, which can in return create SNS notifications
  * Ex: you can create events for failed pipelines
  * Ex: you can create events for cancelled stages
* If CodePipeline fails a stage, your pipeline stops and you can get information in the console
* AWS CloudTrail can be used to audit AWS API calls
* If Pipeline can't perform an action, make sure the "IAM Service Role" attached does have enough permissions (IAM Policy)

### CodeBuild Overview

* Fully managed build service
* Alternative to other build tools such as Jenkins
* Continuous scaling (no servers to manage or provision - no build queue)
* Pay for usage: the time it takes to complete the builds
* Leverages Docker under the hood for reproducible builds
* Possibility to extend capabilities leveraging our own Docker images
* Secure: Integration with KMS for encryption of build artifacts, IAM for build permissions, VPC for network security, and CloudTrail for API calls logging
* Source Code from GitHub / CodeCommit / CodePipeline / S3 ...
* Build instructions can be defined in code (buildspec.yml file)
* Output logs to Amazon S3 & CloudWatch Logs
* Metrics to monitor CodeBuild statistics
* Use CloudWatch Alarms to detect failed builds and trigger notifications (SNS included)
* CloudWatch Events / AWS Lambda as a Glue
* Ability to reproduce CodeBuild locally to troubleshoot in case of errors
* Builds can be defined within CodePipeline or CodeBuild itself
* **CodeBuild supports a lot of environments natively, but in case your environment is not supported, you can generate a docker image and run any environment you like**

### CodeBuild BuildSpec

* buildspec.yml file must be at the **root** of your code
* Define environment variables:
  * Plaintext variables
  * Secure secrets: use SSM Parameter store
* Phases (specify commands to run):
  * Install: install dependencies you may need for your build
  * Pre build: final commands to execute before build
  * **Build: actual build commands**
  * Post build: finishing touches (zip output for example)
* Artifacts: What to upload to S3 (encrypted with KMS)
* Cache: Files to cache (usually dependencies) to S3 for future build speedup

### CodeBuild Local Build

* In case of need of deep troubleshooting beyond logs you can run CodeBuild locally on your desktop
  * Needs to have Docker and CodeBuild Agent installed

### CodeDeploy Overview

* AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises
* Each EC2 Machine (or on-prem) must be running the CodeDeploy Agent
* The agent is continuously polling AWS CodeDeploy for work to do
* CodeDeploy sends appspec.yml file (must be at the root of the source code)
* Application is pulled from GitHub or S3
* CodeDeploy Agent will report of success / failure of deployment on the instance
* EC2 instances are grouped by deployment group (dev / test / prod)
* Lots of flexibility to define any kind of deployments
* CodeDeploy can be chained into CodePipeline and use artifacts from there
* CodeDeploy can re-use existing setup tools, works with any application, auto scaling integration
* Blue / Green deployments only works with EC2 instances (not on premises)
* Support for AWS Lambda deployments
* CodeDeploy does not provision resources

## AWS CodeDeploy Primary Components

* **Application**: unique name
* **Compute platform:** EC2/On-Premise or Lambda
* **Deployment configuration:** Deployment rules for success / failures
  * EC2/On-Premise: you can specify the minimum number of healthy instances for the deployment. 
  * AWS Lambda: specify how traffic is routed to your updated Lambda functions versions.
* **Deloyment group:** group of tagged instances (allows to deploy gradually)
* **Deployment type:** In-place deployment or Blue/green deployment
* **IAM instance profile:** need to give EC2 the permissions to pull from S3 / Github
* **Application revision:** application code + appspec.yml file
* **Service role:** Role for CodeDeploy to perform what it needs
* **Target Revision:** Target deployment application version

## AWS CodeDeploy AppSec

* File section: how to source and copy from S3 / GitHub to filesystem
* Hooks: set of instructions to do to deploy the new version (hooks can have timeouts). The order is:
  * ApplicationStop
  * DownloadBundle
  * BeforeInstall
  * AfterInstall
  * ApplicationStart
  * **ValidateService: really important**

## AWS CodeDeploy Deployment Config

* Configs:
  * One at a time: one instance at a time, one instance fails => deployment stops
  * Half at a time: 50%
  * All at once: quick but no healthy host, downtime. Good for dev environments
  * Custom: min healthy host = 75%
* Failures:
  * Instances stay in "failed state"
  * New deployments will first be deployed to "failed state" instances
  * To Rollback: redeploy old deployment or enable automated rollback for failures
* Deployment Targets:
  * Set of EC2 instances with tags
  * Directly to an ASG
  * Mix of ASG / Tags so you can build deployment segments
  * Customization in scripts with DEPLOYMENT_GROUP_NAME environment variables

## CodeStar Overview

* CodeStar is an integrated solution that regroups: GitHub, CodeCommit, CodeBuild, CodeDeploy, CloudFormation, CodePipeline, CloudWatch
* Helps quickly create "CICD-ready" projects for EC2, Lambda, Beanstalk
* Supported languages: C#, Go, HTML 5, Java, Node.js, PHP, Python, Ruby
* Issue tracking integration with: JIRA / GitHub Issues
* Ability to integrate with Cloud9 to obtain a web IDE (not all regions)
* One dashboard to view all your components
* Free service, pay only for the underlying services and resources
* Limited Customization

___

## **CloudFormation**

* CloudFormation is a declarative way of outlining your AWS Infrastructure, for any resources (most are supported)
* CloudFormation creates declared resources, in the **right order**, with the exact configuration you specify

### Benefits of AWS CloudFormation

* Infrastructure as Code
  * No resources are manually created, which is great for control
  * The code can be version controlled for example, using git
  * Changes to the infrastructure are reviewed through code
* Cost
  * CloudFormation as a service is free, you pay for the resources in the stack
  * Each resources within the stack is stagged with an identifier so you can easily see how much a stack costs you
  * You can estimate the costs of your resources using the CloudFormation template
  * Savings strategy: In Dev, you could automation deletion of templates at 5 PM and recreated at 8 AM, safely
* Productivity
  * Ability to destroy and re-create an infrastructure on the cloud on the fly
  * Automated generation of Diagram for your templates!
  * Declarative programming (no need to figure out ordering and orchestration)
* Separation of concern: create many stacks for many apps, and many layers. Ex:
  * VPC stacks
  * Network stacks
  * APP stacks
* Don't re-invent the wheel
  * Leverage existing templates on the web
  * Leverage the documentation

### How CloudFormation Works

* Templates have to be uploaded in S3 then referenced in CloudFormation
* To update a template, we can't edit previous ones. We have to re-upload a new version of the template to AWS
* Stacks are identified by a name
* Deleting a stack deletes every single artifact that was created by CloudFormation

### Deploying CloudFormation templates

* Manual way:
  * Editing templates in the CloudFormation Designer
  * Using the console to input parameters, etc
* Automated way:
  * Editing templates in a YAML file
  * Using the AWS CLI to deploy the templates
  * Recommended way when fully want to automate your flow

### CloudFormation Building Blocks

* Template components:
  * **Resources: your AWS resources declared in the template (Required)**
  * Parameters: the dynamic inputs for your template
  * Mappings: the static variables for your template
  * Outputs: References to what has been created
  * Conditionals: List of conditions to perform resource creation
  * Metadata
* Template helpers:
  * References
  * Functions

### CloudFormation Resources

* The core of the CloudFormation template (**mandatory**)
* Tehy represent the different AWS Components that will be created and configured
* Resources are declared and can reference each other
* AWS handles creation, updates and deletion of resources for us
* Resource types identifiers are of the form:
  * **AWS::aws-product-name::data-type-name**
* All the resources available can be found at: <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-template-resource-type-ref.html>

### CloudFormation Parameters

* What are CloudFormation Parameters?
  * Parameters are a way to provide inputs to your AWS CloudFormation template
  * Useful for:
    * If you want to **reuse** your templates across the company
    * Some inputs can not be determined ahead of time
  * Parameters are extremely powerful, controlled, and can prevent errors from happening in your templates thanks to types
* When should you use a parameter?
  * Ask:
    * Is this CloudFormation resource configuration likely to change in the future? If so, make it a parameter
  * You don't have to re-upload a template to change a parameter
* Parameter Settings: Parameters can be controlled by these settings:
  * Type:
    * String
    * Number
    * CommaDelimitedList
    * List,Type>
    *  AWS Parameter (to help catch invalid values -  match against existing values in the AWS Account)
  * Description
  * Constraints
  * ConstraintDescription (String)
  * Min/MaxLenght
  * Min/MaxValue
  * Defaults
  * AllowedValues (array)
  * AllowedPattern (regexp)
  * NoEcho (Boolean)
* How to reference a Parameter
  * The **Fn::Ref** (in YAML **!Ref**) function can be leveraged to reference parameters
  * Parameters can be used anywhere in a template
  * The function can also reference other elements within the template
  * Concept: Pseudo Parameters
    * AWS Offers us pseudo parameters in any CloudFormation template that are enabled by default and can be used at any time.
    * Format for the list: Reference Value : Example Return Value
      * AWS::AccountID : 1234567890
      * AWS::NotificationARNs : [arn:aws:sns:us-east-1:123456789012:MyTopic]
      * AWS::NoValue : Does not return a value.
      * AWS::Region : us-east-2
      * AWS::StackId : arn:aws:cloudformation:us-east-1:123456789012:stack/MyStack/1c2fa620-982a-11e3-aff7-50e2416294e0
      * AWS::StackName : MyStack

### CloudFormation Mappings

* What are mappings?
  * Mappings are fixed variables within your CloudFormation template (Hard coded within the template)
  * Useful to differentiate between types of environments (dev vs prod), regions (AWS regions), AMI types, etc.
* Mappings vs Parameters
  * Mappings are great when you know in advance all the values that can be taken and that they can be deduced from variables such as 
    * Region
    * AZ
    * AWS Account
    * Envirnonment (dev vs prod)
    * Etc...
  * Mappings allow safer control over the template
  * Use Parameters when the values are user specific or case specific
* Acessing Mapping Values
  * **Fn::FindInMap** to return a named value from a specific key (in YAML shorthand: **!FindInMap [ MapName, TopLevelKey, SecondLevelKey ]**)

### CloudFormation Outputs

* What are outputs?
  * The Outputs section declares *optional* output values that we can import into other stacks (if you export them first)
  * You can view the outputs in the AWS Console or using the AWS CLI
  * They're very useful for example, if you define a network CloudFormation, and output the variables such as VPC ID and your Subnet IDs
  * It's the best way to perform some collaboration cross stack, as you let each team handle their own part of the slack
  * **You can't delete a CloudFormation Stack if its outputs are being referenced by another CloudFormation stack**

### CloudFormation Conditions

* What are conditions used for?
  * Conditions are used to control the creation of resources or outputs based on a condition
  * Common conditions are:
    * Environment (dev / test / prod)
    * AWS Region
    * Parameter value
  * Each condition can reference another condition, parameter value or mapping
* How to define a condition? Here's an example: 
``` YAML
  Conditions:

  CreateProdResources: !Equals [ !Ref EnvType, prod] 
```
* The logical ID is defined by you, it's how you name the condition
* The intrinsic function (logical) can be any of the following:
  * Fn::And
  * Fn::Equals
  * Fn::If
  * Fn::Not
  * Fn::Or
* Conditions can be applied to resources / outputs / etc...

### CloudFormation Intrinsic Functions

* Must know intrinsic Functions
  * Fn::Ref | !Ref
    * Parameters => returns the value of the parameter
    * Resources => returns the physical ID of the underlying resource
  * Fn::GetAtt | !GetAtt
    * The best practice is to check the documentation of each resource and find out each attribute available for it
  * Fn::FindInMap | !FindInMap [MapName, TopLevelKey, SecondLevelKey ]
  * Fn::ImportValue | !ImportValue
    * Import values that are exported in other templates
  * Fn::Join | !Join
    * Join values with a delimiter
  * Fn::Sub | !Sub
    * Used to substitute variables from a text. 
    * **String** must contain ${VariableName} and will substitute them
  * Condition Functions (listed in the section above)

### CloudFormation Rollbacks

* Stack Creation Fails:
  * Default: everything rolls back (gets deleted). We can check the logs
  * Option to disable rollback and troubleshoot the event
* Stack Update Fails:
  * The stack automatically rolls back to the previous known working state
  * Ability to see in the log what happened and error messages

___

## **AWS Monitoring & Audit: CloudWatch, X-Ray and CloudTrail**

### Monitoring in AWS

* AWS CloudWatch:
  * Metrics: Collect and track key metrics
  * Logs: Collect, monitor, analyze and store log files
  * Events: Send notifications when certain events happen in your AWS
  * Alarms: React in real-time to metrics / events
* AWS X-Ray:
  * Troubleshooting application performance and errors
  * Distributed tracing of microservices
* AWS CloudTrail:
  * Internal monitoring of API calls being made
  * Audit changes to AWS Resources by your users

### AWS CloudWatch Metrics

* CloudWatch provides metrics for every services in AWS
* **Metric** is a variable to monitor (CPUUtilization, NetworkIn...)
* Metrics belongs to namespaces
* **Dimension** is an attribute of a metric (instance id, environment, etc...)
* Up to 10 dimensions per metric
* Metrics have **timestamps**
* AWS CloudWatch EC2 Detailed monitoring
  * EC2 instances metrics are by default updated every 5 minutes, detailed monitoring decreases it to 1 minute, but costs more
  * Use detailed monitoring if you want to more promptly scale your ASG
  * EC2 Memory usage is not a default metric, but a custom one.
* AWS CloudWatch Custom Metrics
  * Possibility to define and send your own custom metrics to CloudWatch
  * Ability to use dimnensions (attributes) to segment metrics
    * Instance.id
    * Environment.name
  * Metric Resolution:
    * Standard: 1 minute
    * High Resolution: up to 1 second (**StorageResolution** API parameter) - Higher cost
  * Use API call **PutMetricData**
  * Use exponential back off in case of throttle errors

### AWS CloudWatch Alarms

* Alarms are used to trigger notifications for any metric
* Alarms can go to Auto Scaling, EC2 Actions, SNS notifications
* Various options (sampling, %, max, min, etc...)
* Alarm States:
  * OK
  * INSUFFICIENT_DATA
  * ALARM
* Period:
  * Lenght of time in seconds to evaluate the metric
  * High resolution custom metrics can only choose 10 or 30 sec

### CloudWatch Logs

* Applications can send logs to CloudWatch using the SDK
* CloudWatch can collect log from:
  * ElasticBeanstalk: collection of logs from application
  * ECS: collection from containers
  * AWS Lambda: collection from function logs
  * VPC Flow Logs: VPC specific logs
  * API Gateway
  * CloudTrail based on filter
  * CloudWatch log agents: for example on EC2 machines or on premises
  * Route53: Log DNS queries
* CloudWatch Logs can go to:
  * Batch exporter to S3 for archival
  * Stream to ElasticSearch cluster for further analytics
* CloudWatch Logs can use filter expressions
* Logs storage architecture:
  * Log groups: arbitrary name, usually representing an application
  * Log stream: instances within application / log files / containers
* Can define log expiration policies (never expire, 30 days, etc...)
* Using the AWS CLI we can tail CloudWatch logs
* To send logs to CloudWatch, make sure IAM permissions are correct!
* Security: encryption of logs using KMS are at the Group Level

### AWS CloudWatch Events

* Schedule: Cron jobs
* Event Pattern: Event rules to react to a service doing something
* Triggers to Lambda functions, SQS / SNS / Kinesis Messages
* CloudWatch Event creates a small JSON document to give information about the change

### AWS X-Ray

* AWS X-Ray provides visual analysis of our applications
* AWS X-Ray advantages:
  * Troubleshooting performance (bottlenecks)
  * Understand dependencies in a microservice architecture
  * Pinpoint service issues
  * Review request behavior
  * Find errors and exceptions
  * Are we meeting time SLA
  * Where I am throttled?
  * Identify users that are impacted
* X-Ray compatibility
  * AWS Lambda
  * Elastic Beanstalk
  * ECS
  * ELB
  * API Gateway
  * EC2 instances or any application servers (even on premises)
* AWS X-Ray Leverages Tracing
  * Tracing is an end to end way to following a "request"
  * Each component dealing with the request adds its own "trace"
  * Tracing is made of segments (+sub segments)
  * Annotations can be added to traces to provide extra-information
  * Ability to trace:
    * Every request
    * Sample request (as a Â¢ for example or a rate per minute)
  * X-Ray Security:
    * IAM for authorization
    * KMS for encryption at rest
* How to enable X-Ray:
  * Your code (Java, Python, Go, Node.js, .NET) must import the AWS X-Ray SDK
  * The application SDK will thencapture:
    * Calls to AWS services
    * HTTP / HTTPS requests
    * Database Calls (MySQL, PostgreSQL, DynamoDB)
    * Queue calls (SQS)
  * Install the X-Ray daemon or enable X-Ray AWS Integration
    * X-Ray daemon works as a low level UDP packet interceptor
    * AWS Lambda / Each application must have the IAM rights to write data to X-Ray
  * To enable on AWS Lambda:
    * Ensure it has an IAM execution role with proper policy (AWSX-RayWriteOnlyAccess)
    * Ensure that X-Ray is imported in the code

### X-Ray Exam tips

* The X-Ray daemon / agent has a config to send traces cross account. This allows to have a central account for all your application tracing:
  * Make sure that the IAM permissions are correct -  the agent will assume the role.
* **Segments**: each application / service will send them
* **Trace**: segments collected together to form an end-to-end trace
* **Sampling**: decrease the amount of requests sent to X-Ray, reduce costs
* **Annotations**: Key Value pairs used to index traces and use with filters
* **Metadata**: Key value pairs, **not indexed**, not used for searching
* Code must be instrumented to use AWS X-Ray SDK (interceptors, handlers, http clients)
* IAM role must be correct to send traces to X-Ray
* **X-Ray on EC2 / on Premises**:
  * Linux system must run the X-Ray daemon
  * IAM instance role if EC2, other AWS credentials on on-premise instance
* **X-Ray on Lambda**:
  * Make sure X-Ray integration is checked on Lambda
* **X-Ray on Beanstalk**:
  * Set configuration on EB console
  * Or use a beanstalk extension (.ebsextensions/xray-daemon.config)
* **X-Ray on ECS / EKS / Fargate (Docker)**:
  * Create a Docker image that runs the Daemon / or use the official X-Ray Docker image
  * Ensure port mappings & network settings are correct and IAM task roles are defined

### AWS CloudTrail

* Provides cgovernance, compliance and audit for your AWS Account
* CloudTrail is enabled by default
* Get an histopy of events / API calls made within your AWS Account by
  * Console
  * SDK
  * CLI
  * AWS Services
* Can put logs from CloudTrail into CloudWatch Logs
* If a resource is deleted in AWS, check CloudTrail

### CloudTrail vs CloudWatch vs X-Ray

* CloudTrail
  * Audit API calls made by users / services / AWS console
  * Useful to detect unauthorized calls or root cause of changes
* CloudWatch:
  * CloudWatch Metrics over time for monitoring
  * CloudWatch Logs for storing application log
  * CloudWatch Alarms to send notifications in case of unexpected metrics
* X-Ray:
  * Automated Trace Analysis & Central Service Map Visualization
  * Latency, Errors and Fault analysis
  * Request tracking accross distributed systems

___
## **AWS Integration & Messaging: SQS, SNS & Kinesis**

### Section introduction

* When we start deploying multiple applications, they will inevitably need to communicate with one another
* There are two patterns of application communicaton
  * Synchronous commucation (application to application)
    * Can be problematic if there are sudden spikes of traffic
  * Asynchronous / Event based (application to queue to application)
    * Better to handle sudden spikes:
      * Using SQS: Queue model
      * Using SNS: pub/sub model
      * using Kinesis: real-time streaming model

### AWS SQS (Simple Queue Service)

* AWS SQS - Standard Queue
  * Oldest offering (over 10 y.o.)
  * Fully managed
  * Scales from 1 message per second to 10000s per second
  * Default retention of messages: 4 days, maximum of 14 days
  * No limit to how many messages can be in the queue
  * Low latency (<10ms on publish and receive)
  * Horizontal scaling in terms of number of consumers
  * Can have duplicate messages (at least once delivery, occasionally)
  * Can have out of order messages (best effort ordering)
  * Limitation of 256KB per message sent
* AWS SQS - Delay Queue
  * Delay a message (consumers don't see it immediately) up to 15 minutes
  * Default is 0 seconds (message available right away)
  * Can set a default at queue level
  * Can override the default using the DelaySeconds parameter
* SQS - Producing Messages
  * Define body (string, up to 256 KB)
  * Add message attributes (metadata - optional)
  * Provide Delay Delivery (optional)
  * After the message is sent, you receive:
    * Message identifier
    * MD5 hash of the body
* SQS - Consuming messages
* Consumers:
  * Poll SQS for messages (receive up to 10 messages at a time)
  * Process the message within the visibility timeout
  * Delete the message using the message ID & receipt handle
* SQS - Visibility timeout
  * When a consumer polls a message from a queue, the message is "invisibile" to other consumers for a defined period, the **Visibility Timeout**
    * Between 0 seconds and 12 hours (default is 30s)
    * If too high (15 minutes) and consumer fails to process the message, you must wait a long time before processing the message again
    * If too low (30s) and consumer needs to process the message (2m), another consumer will receive the message and it will be processed more than once
  * **ChangeMessageVisibility** API to change the visibility while processing a message
  * **DeleteMessage** API to tell SQS the message was successfully processed
* AWS SQS - Dead Letter Queue
  * If a consumer fails to process a message within the Visibility Timeout, the message goes back to the queue
  * We can set a threshold of how many times a message can go back to the queue - it's called a "redrive policy"
  * After the message threshold is exceeded, the message goes into a dead letter queue (DLQ)
  * We have to create a DLQ first and then designate it dead letter queue
  * Make sure to process the messages in the DLQ before they expire
* AWS SQS - Long Polling
  * When a consumer requests message from the queue, it can optionally "wait" for messages to arrive if there are none in the queue - this is called Long Polling
  * **LongPolling decreasese the number of API calls made to SQS while increasing the efficiency and latency of your application**
  * The wait time can be between 1s to 20s (20s preferable)
  * Long Polling is preferable to Short Polling
  * Long polling can be enabled at the queue level or at the API level using **WaitTimeSeconds**

### AWS SQS - FIFO Queue

* Newer offering - Not available in all regions
* Name of the queue must end in .fifo
* Lower throughput (up to 3000 per second with batching, 300/s without)
* Messages are processed in order by the consumer
* Messages are sent exactly once
* No per message delay (only once per queue delay)
* SQS FIFO - Features
  * **Deduplication**: (not send the same message twice)
    * Provide a MessageDeduplicationId with your message
    * De-duplication interval is 5 minutes, so if in 5 minutes there are repeated messages 
    * Content based duplication: the MessageDeduplicationId is generated as the SHA-256 of the message body (not the attributes)
  * **Sequencing**:
    * To ensure strict ordering between messages, specify a **MessageGroupId**
    * Messages with different Group ID may be received out of order
    * E.G. to order messages for a user, you could use the "user_id" as a group id
    * Messages with the same Group ID are delivered to one consumer at a time
* SQS Extended Client
  * Message size limit is 256KB, how to send large messages? Using the SQS Extended Client (Java Library)
  * The producer sends the content to a S3 bucket and a small metadata message to the Queue, the consumer then receives the metadata message, and pulls the larger file from S3.(Access to the S3 bucket is necessary)
* AWS SQS Security
  * Encryption in flight using the HTTPS endpoint
  * Can enable SSE (Server Side Encryption) using KMS
    * Can set the CMK (Customer Master Key) we want to use
    * Can set the data key reuse period (between 1 minute and 24 hours)
      * Lower and KMS API will be used often (higher security, higher cost)
      * Higher and KMS will be used less (Lower Cost, lower security)
    * SSE only encrypts the body, not the metadata (message ID, timestamp, attributes)
  * IAM policy must allow the usage of SQS
  * SQS queue access policy
    * Finer grained control over IP
    * Control over the time the requests come in
  * No VPC Endpoint, must have internet access to access SQS
* SQS - Must know API
  * CreateQueue, DeleteQueue
  * PurgeQueue: delete all the messages in queue
  * SendMessage, ReceiveMessage, DeleteMessage
  * ChangeMessageVisibility: change the timeout
  * Batch APIs for SendMessage, DeleteMessage, ChangeMessageVisibility, helps decrease your costs (BatchSendMessage, BatchDeleteMessage, etc...)
  * No BatchReceiveMessage, you can receive up to 10 messages

### AWS SNS

* The "event producer" only sends message to one SNS topic
* As many "event receivers" (subscriptions) as we want to listen to the SNS topic notifications
* Each subscriber to the topic will get all the messages(new feature to filter messages, might not be present in the exam)
* Up to 10000000(10 million) subscriptions per topic
* 100000 topics limit
* Subscribers can be:
  * SQS
  * HTTP / HTTPS (with delivery retries - how many times)
  * Lambda
  * Emails
  * SNS messages
  * Mobile Notifications
* SNS integrates with several Amazon products:
  * Some services can send data directly to SNS for notifications
  * CloudWatch (for alarms)
  * ASG notifications
  * Amazon S3 (on bucket events)
  * CloudFormation (upon stage changes => failed to build, etc)
  * Etc...
* AWS SNS - How to publish
  * Topic Publish (within your AWS Server - using the SDK)
    * Create a topic
    * Create a subscription (or many)
    * Publish to the topic
  * Direct Publish (for mobile apps SDK)
    * Create a platform application
    * Create a platform endpoint
    * Publish to the platform endpoint
    * Works with Google GCM, Apple APNS, Amazon ADM...
* SNS + SQS: Fan Out
  * Push once in SNS, receive in many SQS
  * Fully decoupled
  * No data loss
  * Ability to add receivers of data later
  * SQS allows for delayed processing (SNS needs instant processing)
  * SQS allows for retries of work
  * May have many workers on one queue and one worker on the other queue

### AWS Kinesis

* **Kinesis** is a managed alternative to Apache Kafka
* Great for:
  * Application logs, metrics, IoT, clickstreams
  * "Real-time" big data
  * Streaming processing frameworks (Spark, NiFi, etc...)
  * Data is automatically replicated to 3 AZ
* **Kinesis Streams**: low latency streaming ingest at scale
  * Streams are divided in ordered Shards / Partitions (scaling up addings shards)
  * **Kinesis Stream Shards**
    * One stream is made of many different shards
    * 1MB/s or 1000messages/s at write **PER SHARD**
    * 2MB/s at read **PER SHARD**
    * Billing is per shard provisioned, can have as many shards as you want
    * Batching available or per message calls
    * The number of shards can evolve over time (reshard / merge)
    * **Records are ordered per shard**
  * Data retention is 1 day by default, can go up to 7 days
  * Ability to reprocess / replay data
  * Multiple applications can consume the same stream
  * Real-time processing with scale of throughput
  * Once data is inserted in Kinesis, it can't be deleted (immutability)
* **Kinesis Analytics**: perform real-time analytics on streams using SQL
* **Kinesis Firehose**: load streams into S3, Redshift, ElasticSearch...
* AWS Kinesis API
  * **AWS Kinesis API - Put Records**
    * PutRecord API + Partition key that gets hashed
    * The same key goes to the same partition (helps with ordering for a specific key)
    * Choose a partition key that is highly distributed ( helps prevent "hot partition")
      * user_id if many users
      * **Not** country_id if 90% of the users are in one country
    * Use Batching with PutRecords to reduce costs and increase throughput
    * **AWS Kinesis API - ProvisionedThroughputExceeded** if we go over the limits
    * Can use CLI, AWS SDK, or producer libraries from various frameworks
  * **AWS Kinesis API - Exceptions**
    * ProvisionedThroughputExceeded Exceptions
      * Happens when sending more data (exceeding MB/s or TPS for any shard)
      * Make sure you don't have a hot shard (such as your partition key is bad and too much data goes to that partition)
      * Solution:
        * Retries with backoff
        * Increase shards (scaling)
        * Ensure your partition key is a good one
  * **AWS Kinesis API - Consumers**
    * Can use a normal consumer (CLI, SDK, etc...)
    * Can use Kinesis Client Library (in Java, Node, Python, Ruby, .Net)
      * KCL uses DynamoDB to checkpoint offsets
      * KCL uses DynamoDB to track other workers and share the work amongst shards

### Kinesis KCL

* Kinesis Client Library (KCL) is a Java library that helps read record for a Kinesis Streams with distributed applications sharing the read workload
* **Rule: each shard is to be read by only one KCL instance (but a KCL instance can read more than one Shard)**
* Means 4 shards =  max 4 KCL instances
* Progress is checkpointed into DynamoDB
* KCL can run on EC2, Elastic Beanstalk, on Premise Application
* **Records are read in order at the shard level**

### Kinesis Security

* Control access / authorization using IAM policies
* Encryption in flight using HTTPS endpoints
* Possibility to encrypt / decrypt data client side (harder)
* VPC Endpoints available for Kinesis to access within VPC

### AWS Kinesis Data Analytics

* Perform real-time analytics on Kinesis Streams using SQL
* Kinesis Data Analytics:
  * Auto Scaling
  * Managed: no servers to provision
  * Continuous: real time
* Pay for actual consumption rate
* Can create streams out of the real-time queries

### AWS Kinesis Firehose

* Fully Managed Service, no administration
* Near Real Time (60s latency)
* Load data into Redshift / Amazon S3 / ElasticSearch / Splunk 
* Automatic scaling
* Support many data format (pay for conversion)
* Pay for the amount of data going through Firehose)

___

## **AWS Serverless: Lambda**

### Lambda Overview

* Virtual **functions** - no servers to manage
* Limited by time - **short executions**
* Run **on-demand**
* **Scalingis automated**
* Benefits of lambda:
  * Easy pricing, pay per request and compute time
  * Integrated with the whole AWS suite of services
  * Integrated with many programming languages
  * Easy monitoring through AWS CloudWatch
  * Easy to get more resources per functions (up to 3GB of RAM)
  * Increasing RAM will also improve CPU and Network
* Main Lambda integrations:
  * API Gateway
  * Kinesis
  * DynamoDB
  * S3
  * CloudFront
  * CloudWatch Events EventBridge
  * CloudWatch Logs
  * SNS
  * SQS
  * Cognito

### Lambda - Synchronous Invocations

* Synchronous: CLI, SDK, API Gateway, Application Load Balancer
  * Results are returned right away
  * Error handling must happen client side (retries, exponential backoff, etc...)
* Synchronous Invocations - Services 
  * User Invoked:
    * Amazon API Gateway
    * Amazon CloudFront (Lambda@Edge)
    * Elastic Load Balancing
    * Amazon S3 Batch
  * Service Invoked:
    * Amazon Cognito
    * AWS Step Functions
  * Other Services
    * Amazon Lex
    * Amazon Alexa
    * Amazon Kinesis Data Firehose

### Lambda Integration with ALB

* To expose a Lambda function as an HTTP(S) endpoint you can use the ALB or API Gateway
* The Lambda function must be registered in a **target group**
* ALB Multi-Header Values
  * ALB can support multi header values (ALB setting)
  * When you enable multi-value headers, HTTP headers and query string parameters that are sent with multiple values are shown as arrays within the AWS Lambda event and response objects. 

### Lambda@Edge

* You have deployed a CDN using CloudFront
* What if you wanted to run a global AWS Lambda alongside, or if you ned to implement request filtering before reaching your application? You can use Lambda@Edge to deploy Lambda alongside CloudFront CDN
  * Build more responsive applications
  * No servers to manage, Lambda is deployed globally
  * Customize the CDN content
  * Pay for what you use
* You can use Lambda to change CloudFront requests and responses:
  * After CloudFront receives a request from a viewer (viewer request)
  * Before CloudFront forwards the request to the origin (origin request)
  * After CloudFront receives the response from the origin (origin response)
  * Before CloudFront forwards the response to the viewer (viewer response)
  * You can also generate responses to viewers without ever sending the request to the origin
* Lambda@Edge: Use Cases
  * Website Security and Privacy
  * Dynamic Web Application at the Edge
  * Search Engine Optimization (SEO)
  * Intelligently Route Across Origins and Data Centers
  * Bot Mitigation at the Edge
  * Real-time Image Transformation
  * A/B Testing
  * User Authentication and Authorization
  * User Priorization
  * User Tracking and Analytics

### Lambda - Asynchronous Invocations

* S3, SNS, CloudWatch Events...
* The events are placed in an **Event Queue**
* Lambda attempts to retry on errors
  * 3 tries total: 1 minute wait after 1st attempt, then 2 minutes wait
* Make sure the processing is idempotent (in case of retries, same results)
* If the function is retried, you will see **duplicate logs entries in CloudWatch Logs**
* Can define a DLQ - SNS or SQS -  for failed processing
* Asynchronous invocations allow you to speed up the processing if you don't need to wait for the result